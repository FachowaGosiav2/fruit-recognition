{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import pathlib \n",
    "from matplotlib import pyplot as plt\n",
    "from import_train_test import Import_path_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, test_path = Import_path_to_data.Import_train_test()\n",
    "folder_list = os.listdir(train_path)\n",
    "\n",
    "def load_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img = cv2.cvtColor(cv2.imread(os.path.join(folder_path, filename)), cv2.COLOR_BGR2RGB)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "def list_the_img(train_path):    \n",
    "    list_off_img = []\n",
    "    for class_ in folder_list:\n",
    "        path_ = pathlib.Path(f'{train_path}\\{class_}')\n",
    "        img = load_images_from_folder(path_)\n",
    "        list_off_img.append(img)\n",
    "    return list_off_img\n",
    "\n",
    "def X_y(list_off_img): \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for index_1 in range(0, len(list_off_img)):\n",
    "        for index_2 in list_off_img[index_1]:\n",
    "            X.append(index_2)\n",
    "            y.append(folder_list[index_1])\n",
    "            \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, y_train = X_y(list_the_img(train_path))\n",
    "X_test_raw, y_test = X_y(list_the_img(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train_raw, axis=-1).astype(np.float32) / 255\n",
    "X_test = np.expand_dims(X_test_raw, axis=-1).astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Dense, MaxPool2D, Flatten, Dropout\n",
    "from keras import Sequential\n",
    "from keras.activations import relu, softmax\n",
    "from keras.optimizers import Nadam\n",
    "from keras.initializers import he_normal\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.backend import clear_session\n",
    "from tensorflow.random import set_seed\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(Conv2D, kernel_size=3, padding=\"same\",\n",
    "                        activation=relu, kernel_initializer=he_normal)\n",
    "model = Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[100, 100, 4]),\n",
    "    MaxPool2D(),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    MaxPool2D(),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(units=128, activation=relu,\n",
    "                          kernel_initializer=he_normal),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=64, activation=relu,\n",
    "                          kernel_initializer=he_normal),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=10, activation=softmax)\n",
    "])\n",
    "\n",
    "optimizer = Nadam(learning_rate=0.1e-3,)\n",
    "model.compile(loss=sparse_categorical_crossentropy, optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'str'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m,\n\u001b[0;32m      2\u001b[0m                     validation_batch_size\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m score \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test)\n\u001b[0;32m      4\u001b[0m X_new \u001b[39m=\u001b[39m X_test[:\u001b[39m10\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\gracj\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\gracj\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:1083\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1080\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1081\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1083\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1084\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1085\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(_type_name(x), _type_name(y))\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'str'>\"})"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=15,\n",
    "                    validation_batch_size=0.1)\n",
    "score = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:10]\n",
    "y_pred = model.predict(X_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
